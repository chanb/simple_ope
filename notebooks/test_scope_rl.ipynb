{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scope_rl.dataset import SyntheticDataset\n",
    "from scope_rl.policy import EpsilonGreedyHead\n",
    "# import d3rlpy algorithms\n",
    "from d3rlpy.algos import DoubleDQNConfig\n",
    "from d3rlpy.dataset import create_fifo_replay_buffer\n",
    "from d3rlpy.algos import ConstantEpsilonGreedy\n",
    "# import rtbgym and gym\n",
    "import d3rlpy\n",
    "import os\n",
    "import rtbgym\n",
    "import gym\n",
    "import torch\n",
    "# random state\n",
    "random_state = 12345\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0) Setup environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "load_path = \"./d3rlpy_logs/DoubleDQN_online_20241104073548/model_100000.d3\"\n",
    "\n",
    "if os.path.isfile(load_path):\n",
    "    ddqn = d3rlpy.load_learnable(load_path)\n",
    "else:\n",
    "    # (1) Learn a baseline policy in an online environment (using d3rlpy)\n",
    "    # initialize the algorithm\n",
    "    ddqn = DoubleDQNConfig().create(device=device)\n",
    "    # train an online policy\n",
    "    # this takes about 5min to compute\n",
    "    ddqn.fit_online(\n",
    "        env,\n",
    "        buffer=create_fifo_replay_buffer(limit=10000, env=env),\n",
    "        explorer=ConstantEpsilonGreedy(epsilon=0.3),\n",
    "        n_steps=100000,\n",
    "        n_steps_per_epoch=1000,\n",
    "        update_start_step=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "offline_dataset_path = \"offline_data-cartpole.dill\"\n",
    "if os.path.isfile(offline_dataset_path):\n",
    "    with open(offline_dataset_path, \"rb\") as f:\n",
    "        result = dill.load(f)\n",
    "        train_logged_dataset = result[\"train_logged_dataset\"]\n",
    "        test_logged_dataset = result[\"test_logged_dataset\"]\n",
    "else:\n",
    "    # (2) Generate a logged dataset\n",
    "    # convert the ddqn policy into a stochastic behavior policy\n",
    "    behavior_policy = EpsilonGreedyHead(\n",
    "        ddqn,\n",
    "        n_actions=env.action_space.n,\n",
    "        epsilon=0.3,\n",
    "        name=\"ddqn_epsilon_0.3\",\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    # initialize the dataset class\n",
    "    dataset = SyntheticDataset(\n",
    "        env=env,\n",
    "        max_episode_steps=getattr(env, \"step_per_episode\", env._max_episode_steps),\n",
    "    )\n",
    "    # the behavior policy collects some logged data\n",
    "    train_logged_dataset = dataset.obtain_episodes(\n",
    "    behavior_policies=behavior_policy,\n",
    "    n_trajectories=10000,\n",
    "    random_state=random_state,\n",
    "    )\n",
    "    test_logged_dataset = dataset.obtain_episodes(\n",
    "    behavior_policies=behavior_policy,\n",
    "    n_trajectories=10000,\n",
    "    random_state=random_state + 1,\n",
    "    )\n",
    "    dill.dump(\n",
    "        dict(\n",
    "            train_logged_dataset=train_logged_dataset,\n",
    "            test_logged_dataset=test_logged_dataset,\n",
    "        ),\n",
    "        open(\"offline_data-cartpole.dill\", \"wb\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement an offline RL procedure using SCOPE-RL and d3rlpy\n",
    "\n",
    "# import d3rlpy algorithms\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from d3rlpy.algos import DiscreteCQLConfig\n",
    "\n",
    "# (3) Learning a new policy from offline logged data (using d3rlpy)\n",
    "# convert the logged dataset into d3rlpy's dataset format\n",
    "offlinerl_dataset = MDPDataset(\n",
    "    observations=train_logged_dataset[\"state\"],\n",
    "    actions=train_logged_dataset[\"action\"],\n",
    "    rewards=train_logged_dataset[\"reward\"],\n",
    "    terminals=train_logged_dataset[\"done\"],\n",
    ")\n",
    "\n",
    "offline_load_path = \"./d3rlpy_logs/DiscreteCQL_20241104080702/model_10000.d3\"\n",
    "if os.path.isfile(offline_load_path):\n",
    "    cql = d3rlpy.load_learnable(offline_load_path)\n",
    "else:\n",
    "    # initialize the algorithm\n",
    "    cql = DiscreteCQLConfig().create(device=device)\n",
    "    # train an offline policy\n",
    "    cql.fit(\n",
    "        offlinerl_dataset,\n",
    "        n_steps=10000,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a basic OPE procedure using SCOPE-RL\n",
    "\n",
    "# import SCOPE-RL modules\n",
    "from scope_rl.ope import CreateOPEInput\n",
    "from scope_rl.ope import OffPolicyEvaluation as OPE\n",
    "from scope_rl.ope.discrete import DirectMethod as DM\n",
    "from scope_rl.ope.discrete import TrajectoryWiseImportanceSampling as TIS\n",
    "from scope_rl.ope.discrete import PerDecisionImportanceSampling as PDIS\n",
    "from scope_rl.ope.discrete import DoublyRobust as DR\n",
    "\n",
    "# (4) Evaluate the learned policy in an offline manner\n",
    "# we compare ddqn, cql, and random policy\n",
    "cql_ = EpsilonGreedyHead(\n",
    "    base_policy=cql,\n",
    "    n_actions=env.action_space.n,\n",
    "    name=\"cql\",\n",
    "    epsilon=0.0,\n",
    "    random_state=random_state,\n",
    ")\n",
    "ddqn_ = EpsilonGreedyHead(\n",
    "    base_policy=ddqn,\n",
    "    n_actions=env.action_space.n,\n",
    "    name=\"ddqn\",\n",
    "    epsilon=0.0,\n",
    "    random_state=random_state,\n",
    ")\n",
    "random_ = EpsilonGreedyHead(\n",
    "    base_policy=ddqn,\n",
    "    n_actions=env.action_space.n,\n",
    "    name=\"random\",\n",
    "    epsilon=1.0,\n",
    "    random_state=random_state,\n",
    ")\n",
    "evaluation_policies = [cql_, ddqn_, random_]\n",
    "# create input for the OPE class\n",
    "prep = CreateOPEInput(\n",
    "    env=env,\n",
    ")\n",
    "input_dict = prep.obtain_whole_inputs(\n",
    "    logged_dataset=test_logged_dataset,\n",
    "    evaluation_policies=evaluation_policies,\n",
    "    require_value_prediction=True,\n",
    "    n_trajectories_on_policy_evaluation=100,\n",
    "    random_state=random_state,\n",
    ")\n",
    "# initialize the OPE class\n",
    "ope = OPE(\n",
    "    logged_dataset=test_logged_dataset,\n",
    "    ope_estimators=[DM(), TIS(), PDIS(), DR()],\n",
    ")\n",
    "# perform OPE and visualize the result\n",
    "ope.visualize_off_policy_estimates(\n",
    "    input_dict,\n",
    "    random_state=random_state,\n",
    "    sharey=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "from functools import partial\n",
    "from gym.spaces import Dict, Box\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "from src.common_models import Model, get_activation\n",
    "from src.layer_modules import MLPModule\n",
    "from src.learning_utils import make_update_function\n",
    "from src.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPCritic(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_models,\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        num_outputs=1,\n",
    "        use_batch_norm=False,\n",
    "        use_layer_norm=False,\n",
    "    ):\n",
    "        self.num_models = num_models\n",
    "        # Wide critic\n",
    "        self.mlp = MLPModule(\n",
    "            layers=[2048, 2048, num_outputs],\n",
    "            activation=get_activation(CONST_RELU),\n",
    "            output_activation=get_activation(CONST_IDENTITY),\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            use_layer_norm=use_layer_norm,\n",
    "            use_bias=True,\n",
    "        )\n",
    "\n",
    "        self._observation_space = observation_space\n",
    "        self._action_space = action_space\n",
    "\n",
    "        self.forward = jax.jit(\n",
    "            self.make_forward([CONST_BATCH_STATS]), static_argnames=[CONST_EVAL]\n",
    "        )\n",
    "        self.intermediates = jax.jit(\n",
    "            self.make_forward([\"intermediates\"], True), static_argnames=[CONST_EVAL]\n",
    "        )\n",
    "\n",
    "    def init(self, model_key):\n",
    "        sample_obs = self._observation_space.sample()\n",
    "        sample_act = self._action_space.sample()\n",
    "        model_keys = jrandom.split(model_key, self.num_models)\n",
    "        params = jax.vmap(self._init, in_axes=[0, None, None])(\n",
    "            model_keys, sample_obs, sample_act\n",
    "        )\n",
    "        return params\n",
    "\n",
    "    def _init(self, model_key, sample_obs, sample_act):\n",
    "        mlp_key = jrandom.split(model_key)[0]\n",
    "\n",
    "        sample_latent = jnp.concatenate(\n",
    "            (\n",
    "                *[val.astype(float) for key, val in sample_obs.items()],\n",
    "                sample_act,\n",
    "            ),\n",
    "            axis=-1,\n",
    "        )\n",
    "\n",
    "        mlp_params = self.mlp.init(mlp_key, sample_latent[None], eval=True)\n",
    "\n",
    "        params = {\n",
    "            CONST_MLP: mlp_params,\n",
    "        }\n",
    "\n",
    "        return params\n",
    "\n",
    "    def make_forward(\n",
    "        self,\n",
    "        mutable,\n",
    "        capture_intermediates=False,\n",
    "    ):\n",
    "        def forward(\n",
    "            params,\n",
    "            obs,\n",
    "            act,\n",
    "            eval=False,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            # NOTE: Assume batch size is first dim\n",
    "            latent = jnp.concatenate(\n",
    "                (\n",
    "                    *[val.astype(float) for key, val in obs.items()],\n",
    "                    act,\n",
    "                ),\n",
    "                axis=-1,\n",
    "            )\n",
    "            (out, mlp_updates) = self.mlp.apply(\n",
    "                params[CONST_MLP],\n",
    "                latent,\n",
    "                eval=eval,\n",
    "                mutable=mutable,\n",
    "                capture_intermediates=capture_intermediates,\n",
    "            )\n",
    "            return out, {\n",
    "                CONST_MLP: mlp_updates,\n",
    "            }\n",
    "\n",
    "        return forward\n",
    "\n",
    "    def update_batch_stats(self, params, batch_stats):\n",
    "        if CONST_BATCH_STATS in batch_stats[CONST_MLP]:\n",
    "            params[CONST_MLP][CONST_BATCH_STATS] = batch_stats[CONST_MLP][\n",
    "                CONST_BATCH_STATS\n",
    "            ]\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = MLPCritic(1, Dict({\"observation\": env.observation_space}), Box(np.zeros(env.action_space.n), np.ones(env.action_space.n)), use_batch_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = qf.init(jrandom.PRNGKey(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "num_epochs = 100000\n",
    "log_interval = 100\n",
    "batch_size = 32\n",
    "discount_factor = 0.99\n",
    "\n",
    "sample_seed = 42\n",
    "learner_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf_opt = optax.adam(lr)\n",
    "opt_state = qf_opt.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    CONST_MODEL: {CONST_QF: params},\n",
    "    CONST_OPT_STATE: {CONST_QF: opt_state},\n",
    "}\n",
    "optimizers = {\n",
    "    CONST_QF: qf_opt,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key = jrandom.PRNGKey(sample_seed)\n",
    "learner_key = jrandom.PRNGKey(learner_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_monte_carlo_returns(rews, dones, gamma):\n",
    "    def _returns(\n",
    "        next_val, transition\n",
    "    ):\n",
    "        rew, done = transition\n",
    "        val = next_val * gamma * (1 - done) + rew\n",
    "        return val, val\n",
    "\n",
    "    return jax.lax.scan(\n",
    "        _returns, 0, np.concatenate((rews, dones), axis=-1), len(rews), reverse=True\n",
    "    )[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step(loss, model_update, optimizer):\n",
    "    def _step(model_dict, batch, keys, *args, **kwargs):\n",
    "        (agg_loss, aux), grads = jax.value_and_grad(loss, has_aux=True)(\n",
    "            model_dict[CONST_MODEL][CONST_QF],\n",
    "            batch,\n",
    "        )\n",
    "\n",
    "        qf_params, qf_opt_state = model_update(\n",
    "            optimizer[CONST_QF],\n",
    "            grads,\n",
    "            model_dict[CONST_OPT_STATE][CONST_QF],\n",
    "            model_dict[CONST_MODEL][CONST_QF],\n",
    "            aux[CONST_UPDATES],\n",
    "        )\n",
    "\n",
    "        aux[CONST_AGG_LOSS] = agg_loss\n",
    "\n",
    "        return {\n",
    "            CONST_MODEL: {CONST_QF: qf_params},\n",
    "            CONST_OPT_STATE: {CONST_QF: qf_opt_state},\n",
    "        }, aux\n",
    "\n",
    "    return jax.jit(_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sarsa_loss(discount_factor, qf):\n",
    "    def sarsa_loss(qf_params, batch):\n",
    "        all_q_preds, updates = jax.vmap(\n",
    "            partial(qf.forward, eval=False), in_axes=[0, None, None]\n",
    "        )(\n",
    "            qf_params,\n",
    "            jax.tree_util.tree_map(\n",
    "                lambda *args: jnp.concatenate(args, axis=0),\n",
    "                batch[\"observation\"],\n",
    "                batch[\"next_observation\"],\n",
    "            ),\n",
    "            jnp.concatenate(\n",
    "                (batch[\"action\"], batch[\"next_action\"]),\n",
    "                axis=0,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        all_q_preds_min = jax.lax.stop_gradient(jnp.min(all_q_preds, axis=0))\n",
    "        curr_q_preds_min, next_q_preds_min = jnp.split(all_q_preds_min, 2)\n",
    "        curr_q_preds, _ = jnp.split(all_q_preds, 2, axis=1)\n",
    "\n",
    "        # Compute min. clipped TD error\n",
    "        next_vs = next_q_preds_min\n",
    "        curr_q_targets = (\n",
    "            batch[\"reward\"]\n",
    "            + discount_factor * (1 - batch[\"terminal\"]) * next_vs\n",
    "        )\n",
    "        td_errors = (curr_q_preds - curr_q_targets[None]) ** 2\n",
    "        return jnp.mean(td_errors), {\n",
    "            CONST_UPDATES: updates,\n",
    "        }\n",
    "\n",
    "    return jax.jit(sarsa_loss)\n",
    "\n",
    "def make_mc_loss(discount_factor, qf):\n",
    "    def mc_loss(qf_params, batch):\n",
    "        all_q_preds, updates = jax.vmap(\n",
    "            partial(qf.forward, eval=False), in_axes=[0, None, None]\n",
    "        )(\n",
    "            qf_params,\n",
    "            jax.tree_util.tree_map(\n",
    "                lambda *args: jnp.concatenate(args, axis=0),\n",
    "                batch[\"observation\"],\n",
    "                batch[\"next_observation\"],\n",
    "            ),\n",
    "            jnp.concatenate(\n",
    "                (batch[\"action\"], batch[\"next_action\"]),\n",
    "                axis=0,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        curr_q_preds, _ = jnp.split(all_q_preds, 2, axis=1)\n",
    "\n",
    "        # Compute min. clipped TD error\n",
    "        curr_q_targets = batch[\"return\"]\n",
    "        td_errors = (curr_q_preds - curr_q_targets[None]) ** 2\n",
    "        return jnp.mean(td_errors), {\n",
    "            CONST_UPDATES: updates,\n",
    "        }\n",
    "\n",
    "    return jax.jit(mc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_name = \"sarsa\"\n",
    "# loss_name = \"mc\"\n",
    "\n",
    "if loss_name == \"sarsa\":\n",
    "    loss_fn = make_sarsa_loss(discount_factor, qf)\n",
    "elif loss_name == \"mc\":\n",
    "    loss_fn = make_mc_loss(discount_factor, qf)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "step = make_step(loss_fn, make_update_function(qf), optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_mean = []\n",
    "losses_std = []\n",
    "losses = []\n",
    "\n",
    "train_rets = scan_monte_carlo_returns(train_logged_dataset[\"reward\"][..., None], train_logged_dataset[\"terminal\"][..., None], discount_factor)\n",
    "for epoch_i in tqdm(range(num_epochs)):\n",
    "    learner_key = jrandom.fold_in(learner_key, epoch_i)\n",
    "    sample_key = jrandom.fold_in(sample_key, epoch_i)\n",
    "    sample_inds = jrandom.randint(\n",
    "        sample_key,\n",
    "        shape=(batch_size,),\n",
    "        minval=0,\n",
    "        maxval=train_logged_dataset[\"size\"]\n",
    "    )\n",
    "\n",
    "    next_sample_inds = jnp.clip(sample_inds + 1, min=0, max=train_logged_dataset[\"size\"] - 1)\n",
    "    batch = {\n",
    "        \"observation\": {\"observation\": train_logged_dataset[\"state\"][sample_inds]},\n",
    "        \"next_observation\": {\"observation\": train_logged_dataset[\"state\"][next_sample_inds]},\n",
    "        \"action\": np.eye(train_logged_dataset[\"n_actions\"])[train_logged_dataset[\"action\"][sample_inds]],\n",
    "        \"next_action\": np.eye(train_logged_dataset[\"n_actions\"])[train_logged_dataset[\"action\"][next_sample_inds]],\n",
    "        \"reward\": train_logged_dataset[\"reward\"][sample_inds],\n",
    "        \"terminal\": train_logged_dataset[\"terminal\"][sample_inds],\n",
    "        \"return\": train_rets[sample_inds][..., None],\n",
    "    }\n",
    "\n",
    "    model_dict, aux = step(model_dict, batch, learner_key)\n",
    "    losses.append(aux[CONST_AGG_LOSS])\n",
    "\n",
    "    if (epoch_i + 1) % log_interval == 0:\n",
    "        losses_mean.append(np.mean(losses))\n",
    "        losses_std.append(np.std(losses))\n",
    "        losses = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_mean = np.array(losses_mean)\n",
    "losses_std = np.array(losses_std)\n",
    "\n",
    "x_axis = np.arange(len(losses_mean))\n",
    "plt.plot(x_axis, losses_mean)\n",
    "plt.fill_between(x_axis, losses_mean - losses_std, losses_mean + losses_std)\n",
    "plt.ylim(-1, 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"{loss_name}.dill\"\n",
    "\n",
    "dill.dump(\n",
    "    model_dict,\n",
    "    open(save_path, \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rets = scan_monte_carlo_returns(test_logged_dataset[\"reward\"][..., None], test_logged_dataset[\"terminal\"][..., None], discount_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_errors = []\n",
    "batch_i = 0\n",
    "while True:\n",
    "    start_ind = batch_i * batch_size\n",
    "    end_ind = (batch_i + 1) * batch_size\n",
    "\n",
    "    sample_inds = np.arange(start_ind, end_ind)\n",
    "    batch = {\n",
    "        \"observation\": {\"observation\": test_logged_dataset[\"state\"][sample_inds]},\n",
    "        \"action\": np.eye(test_logged_dataset[\"n_actions\"])[test_logged_dataset[\"action\"][sample_inds]],\n",
    "        \"reward\": test_logged_dataset[\"reward\"][sample_inds],\n",
    "        \"terminal\": test_logged_dataset[\"terminal\"][sample_inds],\n",
    "        \"return\": test_rets[sample_inds][..., None],\n",
    "    }\n",
    "\n",
    "    q_vals, _ = jax.vmap(\n",
    "        partial(qf.forward, eval=False), in_axes=[0, None, None]\n",
    "    )(\n",
    "        model_dict[CONST_MODEL][CONST_QF],\n",
    "        batch[\"observation\"],\n",
    "        batch[\"action\"],\n",
    "    )\n",
    "    \n",
    "    value_error = batch[\"return\"][None] - q_vals\n",
    "    value_errors.append(value_error)\n",
    "\n",
    "    if end_ind >= test_logged_dataset[\"size\"]:\n",
    "        break\n",
    "    batch_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logged_dataset = train_logged_dataset\n",
    "# rets = train_rets\n",
    "\n",
    "logged_dataset = test_logged_dataset\n",
    "rets = test_rets\n",
    "\n",
    "for batch_i in range(10):\n",
    "    start_ind = batch_i * logged_dataset[\"step_per_trajectory\"]\n",
    "    end_ind = (batch_i + 1) * logged_dataset[\"step_per_trajectory\"]\n",
    "\n",
    "    sample_inds = np.arange(start_ind, end_ind)\n",
    "    batch = {\n",
    "        \"observation\": {\"observation\": logged_dataset[\"state\"][sample_inds]},\n",
    "        \"action\": np.eye(logged_dataset[\"n_actions\"])[logged_dataset[\"action\"][sample_inds]],\n",
    "        \"reward\": logged_dataset[\"reward\"][sample_inds],\n",
    "        \"terminal\": logged_dataset[\"terminal\"][sample_inds],\n",
    "        \"return\": rets[sample_inds][..., None]\n",
    "    }\n",
    "\n",
    "    q_vals, _ = jax.vmap(\n",
    "        partial(qf.forward, eval=False), in_axes=[0, None, None]\n",
    "    )(\n",
    "        model_dict[CONST_MODEL][CONST_QF],\n",
    "        batch[\"observation\"],\n",
    "        batch[\"action\"],\n",
    "    )\n",
    "\n",
    "    value_error = batch[\"return\"][None] - q_vals\n",
    "\n",
    "    x_axis = np.arange(logged_dataset[\"step_per_trajectory\"])\n",
    "\n",
    "    plt.plot(x_axis, q_vals[0, :, 0], label=\"Q-pred\")\n",
    "    plt.plot(x_axis, batch[\"return\"][:, 0], label=\"GT\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
